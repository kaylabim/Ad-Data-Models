{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b06486c8",
   "metadata": {},
   "source": [
    "# Prepping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9c0f3066",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "convert object dtypes to something else",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m####### Imputing numerical feature ######\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Edit: because feature importance seems to be REALLY high for the age feature, going to try out different imputations \u001b[39;00m\n\u001b[32m     11\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Multiple Imputation (miceforest)\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmiceforest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmf\u001b[39;00m \n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m kernel = \u001b[43mmf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mImputationKernel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43maddata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_all_iterations_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m     25\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m kernel.mice(iterations=\u001b[32m2\u001b[39m)\n\u001b[32m     27\u001b[39m addata = kernel.complete_data()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Ad data project/venv/lib/python3.13/site-packages/miceforest/imputation_kernel.py:166\u001b[39m, in \u001b[36mImputationKernel.__init__\u001b[39m\u001b[34m(self, data, num_datasets, variable_schema, imputation_order, mean_match_candidates, mean_match_strategy, data_subset, initialize_empty, save_all_iterations_data, copy_data, random_state)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    144\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    145\u001b[39m     data: DataFrame,\n\u001b[32m   (...)\u001b[39m\u001b[32m    161\u001b[39m     random_state: Optional[Union[\u001b[38;5;28mint\u001b[39m, np.random.RandomState]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    162\u001b[39m ):\n\u001b[32m    164\u001b[39m     datasets = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(num_datasets))\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimpute_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvariable_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariable_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_all_iterations_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_all_iterations_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_seed_array\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;66;03m# Model Training / Imputation Order:\u001b[39;00m\n\u001b[32m    176\u001b[39m     \u001b[38;5;66;03m# Variables with missing data are always trained\u001b[39;00m\n\u001b[32m    177\u001b[39m     \u001b[38;5;66;03m# first, according to imputation_order. Afterwards,\u001b[39;00m\n\u001b[32m    178\u001b[39m     \u001b[38;5;66;03m# variables with no missing values have models trained.\u001b[39;00m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m imputation_order \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mascending\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdescending\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Ad data project/venv/lib/python3.13/site-packages/miceforest/imputed_data.py:39\u001b[39m, in \u001b[36mImputedData.__init__\u001b[39m\u001b[34m(self, impute_data, datasets, variable_schema, save_all_iterations_data, copy_data, random_seed_array)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col, series \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.working_data.items():\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mcolumn names must be strings\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m         series.dtype.name != \u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m     ), \u001b[33m\"\u001b[39m\u001b[33mconvert object dtypes to something else\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m     column_names.append(col)\n\u001b[32m     42\u001b[39m     pd_dtypes_orig[col] = series.dtype.name\n",
      "\u001b[31mAssertionError\u001b[39m: convert object dtypes to something else"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "addata = pd.read_csv('ad_click_dataset.csv')\n",
    "addata = addata.drop(['id', 'full_name'], axis=1)\n",
    "\n",
    "####### Imputing numerical feature ######\n",
    "# Edit: because feature importance seems to be REALLY high for the age feature, going to try out different imputations \n",
    "\n",
    "# # Simple imputer\n",
    "# num_imputer = SimpleImputer(strategy = 'median')\n",
    "# addata['age'] = num_imputer.fit_transform(addata[['age']])\n",
    "\n",
    "# # Aribtrary value imputation \n",
    "# addata.fillna({'age': -999}, inplace=True)\n",
    "\n",
    "# Multiple Imputation (miceforest)\n",
    "import miceforest as mf \n",
    "kernel = mf.ImputationKernel(\n",
    "    data = addata, \n",
    "    save_all_iterations_data=True, \n",
    "    random_state=42\n",
    ")\n",
    "kernel.mice(iterations=2)\n",
    "addata = kernel.complete_data()\n",
    "\n",
    "# End Tail Imputation \n",
    "from feature_engine.imputation import EndTailImputer\n",
    "end_tail_imputer = EndTailImputer(imputation_method='gaussian', tail='right', fold=3, variables=['age'])\n",
    "end_tail_imputer.fit(addata)\n",
    "addata = end_tail_imputer.transform(addata)\n",
    "\n",
    "###### Imputing categorical features ######\n",
    "cat_cols = ['gender', 'device_type', 'ad_position', 'browsing_history', 'time_of_day']\n",
    "cat_imputer = SimpleImputer(strategy = 'most_frequent')\n",
    "addata[cat_cols] = cat_imputer.fit_transform(addata[cat_cols])\n",
    "\n",
    "# Encoding categorical variables \n",
    "addata = pd.get_dummies(addata, columns = cat_cols, drop_first= True)\n",
    "display(addata.head())\n",
    "\n",
    "# Splitting data \n",
    "X = addata.drop('click', axis=1)\n",
    "y = addata['click']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0e7bb0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                 float64\n",
       "gender               object\n",
       "device_type          object\n",
       "ad_position          object\n",
       "browsing_history     object\n",
       "time_of_day          object\n",
       "click                 int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1485df7c",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8d1ace39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.723\n",
      "\n",
      "Classification Report (Random Forest):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.45      0.53       707\n",
      "           1       0.74      0.87      0.80      1293\n",
      "\n",
      "    accuracy                           0.72      2000\n",
      "   macro avg       0.70      0.66      0.67      2000\n",
      "weighted avg       0.71      0.72      0.71      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 315  392]\n",
      " [ 162 1131]]\n",
      "\n",
      "AUC-ROC: 0.7524697779688476\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "############## Random forest model \n",
    "rf = RandomForestClassifier(random_state=2)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report (Random Forest):\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"\\nAUC-ROC:\", roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e4d013",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c66fa088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaylakim/Desktop/Ad data project/venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [23:28:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"colample_bytree\", \"object\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Accuracy: 0.718\n",
      "\n",
      "Classification Report (XGBoost):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.62      0.61       707\n",
      "           1       0.79      0.77      0.78      1293\n",
      "\n",
      "    accuracy                           0.72      2000\n",
      "   macro avg       0.69      0.70      0.69      2000\n",
      "weighted avg       0.72      0.72      0.72      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[440 267]\n",
      " [297 996]]\n",
      "\n",
      "AUC-ROC: 0.7744349675272466\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "############### XGBoost algorithm \n",
    "XGBoost_model = XGBClassifier(\n",
    "    object='binary:logistic', \n",
    "    random_state=2, \n",
    "    n_estimators=200, \n",
    "    max_depth=7,\n",
    "    learning_rate=0.2,\n",
    "    subsample=0.8, \n",
    "    colample_bytree=0.8,\n",
    "    scale_pos_weight= float(len(y_train[y_train == 0]) / len(y_train[y_train == 1]))\n",
    ")\n",
    "\n",
    "XGBoost_model.fit(X_train, y_train)\n",
    "y_pred_xgboost = XGBoost_model.predict(X_test)\n",
    "\n",
    "print(\"\\nXGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgboost))\n",
    "print(\"\\nClassification Report (XGBoost):\\n\", classification_report(y_test, y_pred_xgboost))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgboost))\n",
    "print(\"\\nAUC-ROC:\", roc_auc_score(y_test, XGBoost_model.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "#### Using gridsearchcv for best xgboost parameters \n",
    "# param_grid = {\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'n_estimators': [50, 100, 200]\n",
    "# }\n",
    "\n",
    "# grid = GridSearchCV(XGBClassifier(objective='binary:logistic', random_state=2), param_grid, cv=5, n_jobs=-1)\n",
    "# grid.fit(X_train, y_train)\n",
    "# print(\"Best parameters:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda6ed5d",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fa3ab209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Classifier Accuracy: 0.741\n",
      "\n",
      "Classification Report (Decision Tree Classifier):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.49      0.57       707\n",
      "           1       0.76      0.88      0.81      1293\n",
      "\n",
      "    accuracy                           0.74      2000\n",
      "   macro avg       0.72      0.68      0.69      2000\n",
      "weighted avg       0.73      0.74      0.73      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 344  363]\n",
      " [ 155 1138]]\n",
      "\n",
      "AUC-ROC: 0.7331414613121903\n"
     ]
    }
   ],
   "source": [
    "######## Decison Tree Classifier \n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights = {classes[i]: weights[i] for i in range(len(classes))}\n",
    "\n",
    "DTC_model = DecisionTreeClassifier(\n",
    "    criterion = 'entropy', \n",
    "    splitter = 'best', \n",
    "    max_depth = None, \n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=1, \n",
    "    class_weight=None, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "DTC_model.fit(X_train, y_train)\n",
    "y_pred_DTC = DTC_model.predict(X_test)\n",
    "\n",
    "print(\"\\nDecision Tree Classifier Accuracy:\", accuracy_score(y_test, y_pred_DTC))\n",
    "print(\"\\nClassification Report (Decision Tree Classifier):\\n\", classification_report(y_test, y_pred_DTC))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_DTC))\n",
    "print(\"\\nAUC-ROC:\", roc_auc_score(y_test, DTC_model.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "# ########## Again, using gridsearchcv for parameter tuning \n",
    "# param_grid = {\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'max_depth': [3, 5, 7, 10, None],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'class_weight': ['balanced', class_weights, None]\n",
    "# }\n",
    "\n",
    "# grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "# # print(grid.best_params_, grid.best_estimator_)\n",
    "# best_clf = grid.best_estimator_\n",
    "# print(accuracy_score(y_test, best_clf.predict(X_test)))\n",
    "\n",
    "# ########## Checking for feature importance \n",
    "# importances = DTC_model.feature_importances_\n",
    "# feature_importance = pd.DataFrame({'feature': X.columns, 'importance': importances})\n",
    "# print(feature_importance.sort_values('importance', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6f9b0c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# # Assuming X and y are already defined\n",
    "# models = [\n",
    "#     ('LR', LogisticRegression()),\n",
    "#     ('LDA', LinearDiscriminantAnalysis()),\n",
    "#     ('KNN', KNeighborsClassifier()),\n",
    "#     ('CART', DecisionTreeClassifier()),\n",
    "#     ('RF', RandomForestClassifier()),\n",
    "#     ('NB', GaussianNB()),\n",
    "#     ('SVM', SVC())\n",
    "# ]\n",
    "\n",
    "# results = []\n",
    "# names = []\n",
    "# for name, model in models:\n",
    "#     kfold = KFold(n_splits=10, shuffle=True, random_state=2)\n",
    "#     cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     print(f\"{name}: {cv_results.mean():.3f} ({cv_results.std():.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9797af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d6a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
